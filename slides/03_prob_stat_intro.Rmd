---
title: "Bayesian data analysis & cognitive modeling"
subtitle: "03: probability & statistics primer"
author: "Michael Franke"
output:
  ioslides_presentation:
    css: mistyle.css
    smaller: no
    transition: faster
    widescreen: yes
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, dev.args = list(bg = 'transparent'), fig.align='center')
require('tidyverse')
require('forcats')
theme_set(theme_bw() + theme(plot.background=element_blank()) )
```

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

```{r, child = "miincludes.Rmd"}

```




## road map for today

<span style = "color:white"> &nbsp; </span>

- probability
    - discrete, continuous, cumulative
    - subjective vs. objective chance
    - means, modes, medians
    
<span style = "color:white"> &nbsp; </span>

- conditional probability & Bayes rule

<span style = "color:white"> &nbsp; </span>

- $p$-values & confidence intervals

# probability: discrete

## definition

<span style = "color:white"> &nbsp; </span>


a <span style = "color:firebrick">discrete probability distribution</span> over a finite set of mutually exclusive world states $\States$ is a function $P \colon \States \rightarrow [0;1]$ such that $P(\States)=1$.

<span style = "color:white"> &nbsp; </span>

for finite $\States$, $P(\state)$ is $\state$'s <span style = "color:firebrick">probability mass</span>

## example

okay, winter is coming; but who will sit the Iron Throne next spring?

<span style = "color:white"> &nbsp; </span>

```{r}
house.probs = c(6,3,1,2,4) %>% (function(x) x/sum(x))
names(house.probs)= c("Targaryen", "Lannister", "Baratheon", "Greyjoy", "Stark")
round(house.probs,3)
```

<span style = "color:white"> &nbsp; </span>

```{r}
sum(house.probs)
```


## example

```{r, fig.height = 3.5}
house.probs.df = as_tibble(house.probs) %>% 
  mutate(house = names(house.probs) %>% factor() %>% fct_inorder()) %>% 
  rename(probability = value)
house.plot = ggplot(house.probs.df, aes(x = house, y = probability)) + 
  geom_bar(stat = "identity", fill = "firebrick")
house.plot
```

## notation

<span style = "color:white"> &nbsp; </span>

if $f \colon \States \rightarrow \mathbb{R}^{\ge0}$, then

$$ P(\state) \propto f(\state) $$

is shorthand notation for

$$ P(\state) = \frac{f(\state)}{ \sum_{\state' \in \States} f(\state')} $$

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">example</span>

```{r, eval = F}
house.probs = c(6,3,1,2,4) %>% (function(x) x/sum(x))
```


## binomial

the <span style = "color:firebrick">binomial distribution</span> gives the probability of observing $k$ successes in $n$ coin flips with a bias of $p$:

$$ B(k ; n,p) = {{n}\choose{k}} p^{k} \, (1-p)^{n-k} $$ 

```{r, echo = F, fig.height = 3.5}
binom.plot.data = expand.grid(n = 24, p = c(0.25, 0.5), k = 0:24) %>% 
  mutate(probability = dbinom(k,n,p), p = as.factor(p))
binom.plot = ggplot(binom.plot.data, aes(x = k, y = probability, fill = p)) + 
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("darkgrey", "firebrick"))
binom.plot
```

## negative binomial

the <span style = "color:firebrick">negative binomial distribution</span> gives the probability of needing $n$ coin flips to observe $k$ successes with a bias of $p$:

$$ NB(n ; k,p) = \frac{k}{n} {{n}\choose{k}} p^{k} \, (1-p)^{n - k}$$

```{r, echo = F, fig.height = 3.5}
neg.binom.plot.data = expand.grid(n = 0:75, p = c(0.25, 0.5), k = 7) %>% 
  mutate(probability = k/n * dbinom(k,n,p), p = as.factor(p))
neg.binom.plot = ggplot(neg.binom.plot.data, aes(x = n, y = probability, fill = p)) + 
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("darkgrey", "firebrick"))
neg.binom.plot
```

## multinomial

probability of seeing $\tuple{x_1, \dots, x_k}$ in $n$ draws from a discrete probability distribution with $\tuple{p_1, \dots, p_n}$ where $x_i$ is the number of times that category $i$ was drawn ($\sum_{i = 1}^k x_i = n$)

$$ \mathrm{MultiNom}(\tuple{x_1, \dots, x_k} ; \tuple{p_1, \dots, p_n} ) = \frac{n!}{x_1! \dots x_n!} p_1^{x_1} \dots p_n^{x_n} $$

## Poisson

<span style = "color:firebrick">Poisson distribution</span> gives the probability of an event occurring $k$ times in a fixed interval, when the expected value and variance of $k$ is $\lambda$

$$ \mathrm{Poisson}(k ; \lambda) =  \frac{\lambda^k \exp(-\lambda)}{k!}$$ 

```{r, echo = F, fig.height = 3.5}
poisson.plot.data = expand.grid(k = 0:25, lambda = c(4, 10)) %>% 
  mutate(probability = dpois(k,lambda), lambda = as.factor(lambda))
poisson.plot = ggplot(poisson.plot.data, aes(x = k, y = probability, fill = lambda)) + 
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("darkgrey", "firebrick"))
poisson.plot
```

## cumulative distribution

$\States$ is <span style = "color:firebrick">ordinal</span> if there is a total strict ordering such that for all $\state, \state' \in \States$:

$$ \state > \state' \ \ \mathrm{or} \ \ \state < \state'$$

the <span style = "color:firebrick">cumulative distribution</span> of $P$ is $P_{\le}(\state) = \sum_{\state' \le \state}P(\state')$

<span style = "color:firebrick">example: cumulative Poisson</span>

```{r, echo = F, fig.height = 3.5}
poisson.plot.data = expand.grid(k = 0:25, lambda = c(4, 10)) %>% 
  mutate(probability = ppois(k,lambda), lambda = as.factor(lambda))
poisson.plot = ggplot(poisson.plot.data, aes(x = k, y = probability, color = lambda)) + 
  geom_point() + geom_line() +
  scale_color_manual(values = c("darkgrey", "firebrick")) + ylab("cumulative probability mass")
poisson.plot
```

## probability logic

<span style = "color:white"> &nbsp; </span>

let $X,Y \subseteq T$

<div style = "float:left; width:35%;">

<span style = "color:firebrick">definition</span>

- $P(X) = \sum_{\state \in X} P(\state)$
  
</div>
<div style = "float:right; width:55%;">

<span style = "color:firebrick">corollaries</span>

- $P(X \cup Y) = P(X) + P(Y) - P(X \cap Y)$
- $P(\States \setminus X) = 1 - P(X)$
  
</div>  


<div style = "position:absolute; top: 620px; right:60px;">
  [more](https://plato.stanford.edu/entries/logic-probability/) on probability & logic
</div>

# probability: continuous

## definition

<span style = "color:white"> &nbsp; </span>

a <span style = "color:firebrick">probability distribution</span> over an infinite set (a convex, continuous interval) $\States \subseteq \mathbb{R}$ is a function $P \colon \pow{\States} \rightarrow \mathbb{R}^{\ge 0}$ such that $\int P(\state) \mathrm{d}\state = 1$

<span style = "color:white"> &nbsp; </span>

for all intervals $I = [a;b] \subseteq \States$: $\Pr(I) = \int_{a}^{b} f(\state) \ \text{d}\state$
    
<span style = "color:white"> &nbsp; </span>

for infinite $\States$, $P(\state)$ is $\state$'s <span style = "color:firebrick">probability density</span>

## Normal distribution

fill me

## beta distribution

fill me

## Dirichlet distribution

fill me

## Gamma distribution

fill me

# conditional probability & Bayes rule

## cond prob

# fini

## to prevent boredom

<span style = "color:firebrick">obligatory</span>

- prepare Kruschke Chapter 4

- fill in [CSV file](https://github.com/michael-franke/BDACM_2017/blob/master/data/01_class_enquette.csv) and send it to [Christian Adam](mailto:c.adam@student.uni-tuebingen.de)

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">optional</span>

- read more on [R for Data Science](http://r4ds.had.co.nz)
    - Chapters 5 and 12 on data representation and wrangling

- explore reading times in today's data set

<span style = "color:white"> &nbsp; </span>

