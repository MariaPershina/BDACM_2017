---
title: "Bayesian data analysis & cognitive modeling"
subtitle: "Session 16: genearlized linear model"
author: "Michael Franke"
output:
  ioslides_presentation:
    css: mistyle.css
    transition: faster
    widescreen: yes
    smaller: yes
---
```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, 
                      dev.args = list(bg = 'transparent'), fig.align='center',
                      cache=TRUE)
require('tidyverse')
require('forcats')
require('rjags')
require('ggmcmc')
require('reshape2')
require('runjags')
require('dplyr')
require('gridExtra')
# require('rstan')
library(GGally)
library(BayesFactor)
# library(brms)

show = function(x) { x }
theme_set(theme_bw() + theme(plot.background=element_blank()) )
```

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  var MML = MathJax.ElementJax.mml,
      TEX = MathJax.InputJax.TeX;

  TEX.Definitions.macros.bfrac = "myBevelFraction";

  TEX.Parse.Augment({
    myBevelFraction: function (name) {
      var num = this.ParseArg(name),
          den = this.ParseArg(name);
      this.Push(MML.mfrac(num,den).With({bevelled: true}));
    }
  });
});
</script>


```{r, child = "miincludes.Rmd"}

```


## overview

<span style = "color:white"> &nbsp; </span>


- generalized linear model (GLM)
    - types of variables
        - metric, nominal, ordinal, count
- linear model
    - ordinary least squares regresssion
    - maximum likelihood regression
    - Bayesian approaches
- generalization to simplest $t$-test scenario

# generalized linear model

## probabilistic models

<span style = "color:white"> &nbsp; </span>


<div style = "float:left; width:45%;">

<span style = "color:firebrick">standard notion</span>

model = likelihood function $P(D \mid \theta)$

</div>
<div style = "float:right; width:45%;">

<span style = "color:firebrick">Bayesian</span>

model = likelihood $P(D \mid \theta)$ + prior $P(\theta)$  

</div>  

<div style = "position:absolute; top: 320px;">

<span style = "color:firebrick">approaches to modeling</span>

<div style = "float:left; width:10%;">
<span style = "color:white"> &nbsp; </span>
</div>
<div style = "float:right; width:85%;">
<div align = 'center'>
<img src="pics/flavors_of_modeling.png" alt="flavors_of_modeling" style="width: 750px;"/>
</div>  
</div>  

</div>  




## generalized linear model

<span style = "color:white"> &nbsp; </span>


<div style = "float:left; width:35%;">
<span style = "color:firebrick">terminology</span>

- $y$ <span style = "color:darkgreen">predicted variable</span>, data, observation, ...
- $X$ <span style = "color:darkgreen">predictor variables</span> for $y$, explanatory variables, ...

<span style = "color:white"> &nbsp; </span>


<span style = "color:firebrick">blueprint of a GLM</span>

$$ 
\begin{align*} 
\eta & = \text{linear_combination}(X)  \\
\mu & = \text{link_fun}( \ \eta, \theta_{\text{LF}} \ )  \\
y & \sim \text{lh_fun}( \ \mu, \ \theta_{\text{LH}} \ )
\end{align*}
$$   
</div>
<div style = "float:right; width:55%;">

<div align = 'center'>
  <img src="//Users/micha/Desktop/data/svn/ProComPrag/teachings/bda+cm2015/slides/pics/glm_scheme/glm_scheme.png" alt="glm_scheme" style="width: 450px;"/>
</div>  
</div>  




## types of variables

<span style = "color:white"> &nbsp; </span>


| type | examples 
|:---|:---|
| metric | speed of a car, reading time, average time spent cooking p.d., ...
| binary | coin flip, truth-value judgement, experience with R, ...
| nominal | gender, political party, favorite philosopher, ... 
| ordinal | level of education, rating scale judgement, ... 
| count | number of cars passing under bridge in 1h, ...

## common link & likelihood function

<span style = "color:white"> &nbsp; </span>


| type of $y$ | (inverse) link function | likelihood function | 
|:---|:---:|:---:|
| metric |  $\mu = \eta$ | $y \sim \text{Normal}(\mu, \sigma)$
| binary | $\mu = \text{logistic}(\eta, \theta, \gamma) = (1 + \exp(-\gamma (\eta - \theta)))^{-1}$ | $y \sim \text{Binomial}(\mu)$
| nominal | $\mu_k = \text{soft-max}(\eta_k, \lambda) \propto \exp(\lambda \eta_k)$ | $y \sim \text{Multinomial}({\mu})$
| ordinal | $\mu_k = \text{threshold-Phi}(\eta_k, \sigma, {\delta})$ | $y \sim \text{Multinomial}({\mu})$
| count | $\mu = \exp(\eta)$ | $y \sim \text{Poisson}(\mu)$

# linear regression

## muder rate data set

<span style = "color:white"> &nbsp; </span>


```{r}
murder_data = readr::read_csv('../data/06_murder_rates.csv') %>% 
  rename(murder_rate = annual_murder_rate_per_million_inhabitants,
         low_income = percentage_low_income, 
         unemployment = percentage_unemployment) %>% 
  select(murder_rate, low_income, unemployment,population)
murder_data %>% head
```

## visualize data

```{r}
GGally::ggpairs(murder_data, 
                title = "Murder rate data")
```

## zooming in

```{r}
murder_sub = select(murder_data, murder_rate, low_income) 
rate_income_plot = ggplot(murder_sub, 
    aes(x = low_income, y = murder_rate)) + geom_point()
rate_income_plot
```

## linear regression

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">given</span>

- predicted variable $y$: murder rates <div style="float: right; margin: 0px;"> metric </div>
- predictor variable $x$: percentage low income <div style="float: right; margin: 0px;"> metric </div> 

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">question</span>

wich linear function --in terms of intercept $\beta_0$ and slope $\beta_1$-- approximates the data best?

$$ y_{\text{pred}} = \beta_0 + \beta_1 x $$ 


## ordinary linear regression

<span style = "color:white"> &nbsp; </span>


<span style = "color:firebrick">"geometric" approach</span>

find $\beta_0$ and $\beta_1$ that minimize the squared error between predicted $y_\text{pred}$ and observed $y$


<span style = "color:white"> &nbsp; </span>

<div style = "float:left; width:45%;">

```{r}
mse = function(y, x, beta_0, beta_1) {
  yPred = beta_0 + x * beta_1
  (y-yPred)^2 %>% mean
}
fit_mse = optim(par = c(0, 1), 
  fn = function(par) with(murder_data,
    mse(murder_rate,low_income, 
        par[1], par[2])))
fit_mse$par
```
  
</div>
<div style = "float:right; width:55%;">

```{r, echo = FALSE, fig.align='center', fig.width=4, fig.height=3}
rate_income_plot + geom_abline(intercept = fit_mse$par[1], 
                               slope = fit_mse$par[2], 
                               color = "firebrick")
```

  
</div>  

## linear regression: MLE

<span style = "color:firebrick">maximum likelihood approach</span>

find $\beta_0$ and $\beta_1$ and $\sigma$ that maximize the likelihood of observed $y$:

$$ 
\begin{align*}
y_{\text{pred}} & = \beta_0 + \beta_1 x  & \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
y & \sim \mathcal{N}(\mu = y_{\text{pred}}, \sigma)
\end{align*}
$$


<span style = "color:white"> &nbsp; </span>

<div style = "float:left; width:45%;">

```{r}
nll = function(y, x, beta_0, beta_1, sd) {
  if (sd <= 0) {return( Inf )}
  yPred = beta_0 + x * beta_1
  nll = -dnorm(y, mean=yPred, sd=sd, log = T)
  sum(nll)
}
fit_lh = optim(par = c(0, 1, 1), 
  fn = function(par) with(murder_data, 
    nll(murder_rate, low_income,
        par[1], par[2], par[3])))
fit_lh$par
```
  
</div>
<div style = "float:right; width:55%;">

```{r, echo = FALSE, fig.align='center', fig.width=4, fig.height=3}
rate_income_plot + geom_abline(intercept = fit_lh$par[1], 
                               slope = fit_lh$par[2], 
                               color = "firebrick")
```

  
</div>  

## compare homebrew to the "real thing"

```{r}
fit_lm = lm(formula = murder_rate ~ low_income, data = murder_data)
fit_glm = glm(formula = murder_rate ~ low_income, data = murder_data)

tibble(parameter = c("beta_0", "beta_1"),
       manual_mse = fit_mse$par,
       manual_lh = fit_lh$par[1:2],
       lm = fit_lm %>% coefficients,
       glm = fit_glm %>% coefficients) %>% show
```


## linear regression: a Bayesian approach

<span style = "color:firebrick">Bayes: likelihood + prior</span>

inspect posterior distribution over $\beta_0$, $\beta_1$ and $\sigma_{\text{err}}$ given the data $y$ and the model:

$$ 
\begin{align*}
y_{\text{pred}} & = \beta_0 + \beta_1 x  & \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
y & \sim \mathcal{N}(\mu = y_{\text{pred}}, \sigma_{err}) \\
\beta_i & \sim \mathcal{N}(0, \sigma_{\beta})  & \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{1}{\sigma_{err}^2} & \sim \text{Gamma}(0.1,0.1)
\end{align*}
$$

```{r, eval = FALSE}
model{
  sigma_e = 1/sqrt(tau_e)
  tau_e ~ dgamma(0.1,0.1)
  b0 ~ dnorm(0, 1/10000000)
  b1 ~ dnorm(0, 1/10000000)
  for (i in 1:k){
    yPred[i] = b0 + b1 * x[i]
    y[i] ~ dnorm(yPred[i], tau_e)
  }
}
```

## linear regression: a Bayesian approach

```{r, echo = FALSE, fig.align='center', fig.width=8, fig.height=4.5}
# specify model
modelString = "
model{
  sigma_e = 1/sqrt(tau_e)
  tau_e ~ dgamma(0.1,0.1)
  b0 ~ dnorm(0, 1/10000000)
  b1 ~ dnorm(0, 1/10000000)
  for (i in 1:k){
    mu[i] = b0 + b1 * x[i]
    y[i] ~ dnorm(mu[i], tau_e)
  }
}
"
# prepare data for JAGS
dataList = list(x= murder_sub$low_income, 
                y = murder_sub$murder_rate,
                k = nrow(murder_sub))

# set up and run model
params <- c('b0', 'b1', 'var_e')
jagsModel = jags.model(file = textConnection(modelString), data = dataList, n.chains = 3, quiet = TRUE)
update(jagsModel, 5000) # 5000 burn-in samples
codaSamples = coda.samples(jagsModel, variable.names = params, n.iter = 20000)

ggmcmc::ggs(codaSamples) %>% 
  group_by(Parameter) %>% 
  summarize(HDI_lo = coda::HPDinterval(as.mcmc(value))[1],
          mean = mean(value),
          HDI_hi = coda::HPDinterval(as.mcmc(value))[2])  %>% 
  show

ggmcmc::ggs_density(ggs(codaSamples))
```
 
# inference with regression models

## inference with regression models

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">parameter estimation</span>

- is coefficient $\beta_i$ significantly/credibly different from, e.g., 0?

<span style = "color:white"> &nbsp; </span>


<span style = "color:firebrick">model comparison</span>

- compare model with $\beta_i$ fixed to model with $\beta_i$ free

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">prediction/model criticism</span>

- predict future data from model
- could model be a likely generating model of the observed data?

## significance of point-estimates of coefficients

```{r}
fit_glm = glm(formula = murder_rate ~ low_income, data = murder_data)
fit_glm %>% summary
```

## credible intervals for posteriors over coefficients

<span style = "color:white"> &nbsp; </span>


```{r}
ggmcmc::ggs(codaSamples) %>% 
  group_by(Parameter) %>% 
  summarize(HDI_lo = coda::HPDinterval(as.mcmc(value))[1],
          mean = mean(value),
          HDI_hi = coda::HPDinterval(as.mcmc(value))[2])  %>% 
  show
```

## model comparison

<span style = "color:firebrick">frequentist</span>

```{r}
fit_glm_simple = glm(formula = murder_rate ~ 1, data = murder_data)
AIC(fit_glm_simple, fit_glm)
```


<span style = "color:firebrick">Bayesian</span>

```{r, fig.align='center', fig.width=8, fig.height=4.5}
# package does not 'like' tibbles
BayesFactor::regressionBF(formula = murder_rate ~ low_income, 
            data = as.data.frame(murder_sub))
```

# $t$-test scenario

## IQ data

```{r}
iq_data = readr::read_csv('../data/07_Kruschke_TwoGroupIQ.csv')
summary(iq_data)
```  

<div style = "float:left; width:45%;">
```{r, echo = FALSE, fig.align='center', fig.width=5, fig.height=2.8}
ggplot(iq_data, aes(x=Group, y = Score)) + geom_boxplot()
```  
</div>
<div style = "float:right; width:45%;">
<span style = "color:firebrick">possible research questions?</span>  

1. is average IQ-score higher than 100 in treatment group?
2. is average IQ-score higher in treatment group than in control? (<span style = "color:darkorange">next time</span>)
</div>  

<div style = "position:absolute; top: 620px; right:60px;">
  from Kruschke (2015, Chapter 16)
</div>

## Case 1: IQ-score higher than 100 in treatment group?

<span style = "color:firebrick">Bayesian GLM:</span>

inspect posterior distribution over $\beta_0$ and $\sigma_{\text{err}}$ given the data $y$ and the model:

$$ 
\begin{align*}
y_{\text{pred}} & = \beta_0  & \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
y & \sim \mathcal{N}(\mu = y_{\text{pred}}, \sigma_{err}) \\
\beta_0 & \sim \mathcal{N}(100, \sigma_{\beta})  & \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{1}{\sigma_{err}^2} & \sim \text{Gamma}(0.1,0.1)
\end{align*}
$$

```{r, eval = FALSE}
model{
  sigma_e = 1/sqrt(tau_e)
  tau_e ~ dgamma(0.1,0.1)
  b0 ~ dnorm(100, 1/10000000)
  for (i in 1:k){
    yPred[i] = b0
    y[i] ~ dnorm(yPred[i], tau_e)
  }
}
```
  

## posterior inference: results

```{r, echo = FALSE, fig.align='center', fig.width=5, fig.height=4}
# specify model
modelString = "
model{
  sigma_e = 1/sqrt(tau_e)
  tau_e ~ dgamma(0.1,0.1)
  b0 ~ dnorm(0, 1/10000000)
  for (i in 1:k){
    mu[i] = b0
    y[i] ~ dnorm(mu[i], tau_e)
  }
}
"
# prepare data for JAGS
iq_score_treatment = filter(iq_data, Group == "Smart Drug")$Score
dataList = list(y = iq_score_treatment, 
                k = length(iq_score_treatment))

# set up and run model
params <- c('b0', 'var_e')
jagsModel = jags.model(file = textConnection(modelString), data = dataList, n.chains = 3, quiet = TRUE)
update(jagsModel, 5000) # 5000 burn-in samples
codaSamples = coda.samples(jagsModel, variable.names = params, n.iter = 20000)

ggmcmc::ggs(codaSamples) %>% 
  group_by(Parameter) %>% 
  summarize(HDI_lo = coda::HPDinterval(as.mcmc(value))[1],
          mean = mean(value),
          HDI_hi = coda::HPDinterval(as.mcmc(value))[2])  %>% 
  show

ggmcmc::ggs_density(ggs(codaSamples))
```

# summary

## generalized linear model

<span style = "color:white"> &nbsp; </span>


<div style = "float:left; width:35%;">
<span style = "color:firebrick">terminology</span>

- $y$ <span style = "color:darkgreen">predicted variable</span>, data, observation, ...
- $X$ <span style = "color:darkgreen">predictor variables</span> for $y$, explanatory variables, ...

<span style = "color:white"> &nbsp; </span>


<span style = "color:firebrick">blueprint of a GLM</span>

$$ 
\begin{align*} 
\eta & = \text{linear_combination}(X)  \\
\mu & = \text{link_fun}( \ \eta, \theta_{\text{LF}} \ )  \\
y & \sim \text{lh_fun}( \ \mu, \ \theta_{\text{LH}} \ )
\end{align*}
$$   
</div>
<div style = "float:right; width:55%;">

<div align = 'center'>
  <img src="//Users/micha/Desktop/data/svn/ProComPrag/teachings/bda+cm2015/slides/pics/glm_scheme/glm_scheme.png" alt="glm_scheme" style="width: 450px;"/>
</div>  
</div>  


## outlook

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">Friday</span>

- robust regression
- GLM with nominal predictors
- GLM with nominal & ordinal predicted variables

<span style = "color:white"> &nbsp; </span>

<span style = "color:firebrick">Tuesday</span>

- mixed effects models
- model comparison by cross-validation

